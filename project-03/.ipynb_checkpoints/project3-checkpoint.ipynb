{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Your Map Area\n",
    "#### san-francisco_california.osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Count all types of tags in the xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {'node': 4482203, 'nd': 5341923, 'bounds': 1, 'member': 43488, 'tag': 1571906, 'osm': 1, 'way': 514729, 'relation': 7026})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Define a function to count all types of tags in the xml file\n",
    "def count_tags(filename):\n",
    "    tags_dict = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        tags_dict[elem.tag] += 1\n",
    "    return tags_dict\n",
    "#Count all tags \n",
    "count_tags(\"san-francisco_california.osm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Method to calculate unique users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3536\n"
     ]
    }
   ],
   "source": [
    "# Count unique  method based on key\n",
    "def count_unique(filename, key):\n",
    "    unique_items = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if key in element.attrib:\n",
    "            unique_items.add(element.get(key))\n",
    "    print(len(unique_items))\n",
    "    \n",
    "# Find all unique user count\n",
    "count_unique(\"san-francisco_california.osm\", \"user\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Main method and helper methods to clean the data and generate json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "# Regex for invalid_chars and street_type\n",
    "invalid_chars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "mapping = {\"St\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"street\": \"Street\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Rd\": \"Road\",\n",
    "           \"Ave\": \"Avenue\",\n",
    "           \"Ave.\": \"Avenue\",\n",
    "           \"Blvd\": \"Boulevard\",\n",
    "           \"Blvd.\": \"Boulevard\",\n",
    "           \"Boulevade\": \"Boulevard\",\n",
    "           \"Cir\": \"Circle\",\n",
    "           \"Cres\": \"Crescent\",\n",
    "           \"Cressent\": \"Crescent\",\n",
    "           \"Crt.\": \"Court\",\n",
    "           \"Dr\": \"Drive\",\n",
    "           \"Dr.\": \"Drive\",\n",
    "           \"Driver\": \"Drive\",\n",
    "           \"Terace\": \"Terrace\"\n",
    "          }\n",
    "\n",
    "#*********************************************************\n",
    "#Common methods used for cleaning and converting data.\n",
    "#*********************************************************\n",
    "##Set element details based on element object\n",
    "def set_element_details(json_obj,element):\n",
    "    set_elements(json_obj,element, [\"id\",\"visible\"])\n",
    "\n",
    "def set_position(json_obj, element):\n",
    "     ## set lon and lat positions as 0 if invalid or not present\n",
    "    json_obj[\"pos\"] = [0, 0]\n",
    "    if element.get(\"lat\"):\n",
    "        try:\n",
    "            json_obj[\"pos\"] = [float(element.get(\"lat\")), float(element.get(\"lon\"))]\n",
    "        except TypeError:  \n",
    "            json_obj[\"pos\"] = [0, 0]\n",
    "\n",
    "def set_sub_element(json_obj,element):\n",
    "    json_obj[\"created\"] = {}\n",
    "    set_elements(json_obj[\"created\"],element,[\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"])\n",
    "\n",
    "def set_elements(json_obj, element, attributes):\n",
    "    for attribute in attributes:\n",
    "        json_obj[attribute] = element.get(attribute) \n",
    "        \n",
    "# split first and second part of the colon.\n",
    "def split_colon(string, pos):\n",
    "    if pos == \"first\":\n",
    "        return string[:(string.index(\":\"))]\n",
    "    elif pos == \"second\":\n",
    "        return string[(string.index(\":\")+1):]\n",
    "# Convert to camel case\n",
    "def camelCase(st):\n",
    "    return ' '.join(''.join([w[0].upper(), w[1:].lower()]) for w in st.split())\n",
    "\n",
    "# Check whether the street name is valid or not\n",
    "def is_valid_street(street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        if m.group() in mapping.keys():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "# Funtion to update street name \n",
    "def update_street_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m and (m.group() in mapping.keys()):\n",
    "        return re.sub(street_type_re, mapping[m.group()], name)\n",
    "    else: \n",
    "        return name\n",
    "\n",
    "# Iterate through all the tags in the element and add it to the dictionary.\n",
    "def build_dictionary(element,json_object):\n",
    "            # Iterate through all the tags in the element to add them into the \"json_object\" dict\n",
    "        for elem in element.iter():\n",
    "            if elem.tag == \"nd\":\n",
    "                json_object[\"node_refs\"].append(elem.get(\"ref\"))\n",
    "            elif elem.tag == \"tag\":\n",
    "                k_val = elem.get(\"k\")\n",
    "                v_val = elem.get(\"v\")\n",
    "\n",
    "                # update the v_val if it's a street name and problem is found\n",
    "                if k_val == \"addr:street\":\n",
    "                    if is_valid_street(v_val):\n",
    "                        v_val = update_street_name(v_val, mapping)\n",
    "                ## Invalid postal code like 豊川市, CA9410, CA, -\n",
    "                if k_val == \"addr:postcode\":\n",
    "                    if v_val:\n",
    "                        v_val = re.sub(\"[^0123456789\\.]\",\"\",str(v_val))\n",
    "                if k_val == \"addr:city\":\n",
    "                    if v_val:\n",
    "                         if not isinstance(v_val, unicode):\n",
    "                            v_val = unicode(v_val, 'utf-8')\n",
    "                            v_val = camelCase(str(v_val))\n",
    "                if invalid_chars.match(k_val):\n",
    "                    continue\n",
    "                elif \":\" in k_val:\n",
    "                    if len(re.findall(\":\", k_val)) > 1: \n",
    "                        continue\n",
    "                    else: \n",
    "                        json_object[split_colon(k_val, \"first\")][split_colon(k_val, \"second\")] = v_val\n",
    "                elif k_val in json_object.keys():\n",
    "                    json_object[k_val][k_val] = v_val\n",
    "                else: \n",
    "                    json_object[k_val] = v_val\n",
    "\n",
    "        return json_object\n",
    "# Convert xml element to json object\n",
    "def convert_to_json(element):\n",
    "    # Initialize json object \n",
    "    json_object = {}\n",
    "    #If element tag is node and way\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        # Set element tag\n",
    "        json_object[\"tag\"] = element.tag\n",
    "        \n",
    "        # set element details based on element object.\n",
    "        set_element_details(json_object,element)\n",
    "        \n",
    "        # set element attributes based on element object.\n",
    "        set_sub_element(json_object,element)\n",
    "\n",
    "        if element.findall('nd'):\n",
    "            json_object[\"node_refs\"] = []\n",
    "\n",
    "        set_position(json_object,element)\n",
    "        \n",
    "        key_set = set()\n",
    "        for tag in element.iter(\"tag\"): \n",
    "            k_val = tag.get(\"k\")\n",
    "            if \":\" in k_val:\n",
    "                key_set.add(split_colon(k_val, \"first\")) \n",
    "        # Initialize dictionary\n",
    "        for key in key_set: \n",
    "            json_object[key] = {}\n",
    "            \n",
    "        build_dictionary(element,json_object)\n",
    "        return json_object   \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Main method to read osm file and convert to json file\n",
    "def convert_osm_to_json(file_name):\n",
    "    #Create a json file based on the osm file name\n",
    "    json_file = \"{0}.json\".format(file_name)\n",
    "    data = []\n",
    "    with codecs.open(json_file, \"w\") as file_out:\n",
    "        #Iterate through all the xml elements in the file\n",
    "        for _, element in ET.iterparse(file_name):\n",
    "            #Convert each element to json object\n",
    "            el = convert_to_json(element)\n",
    "            #if json object is present write to json file\n",
    "            if el:\n",
    "                file_out.write(json.dumps(el, indent=5)+\"\\n\")\n",
    "    return data\n",
    "\n",
    "# Method call to convert osm to json\n",
    "convert_osm_to_json(\"san-francisco_california.osm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oakland\n"
     ]
    }
   ],
   "source": [
    "print camelCase(\"OAKLAND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Problems encounterd \n",
    "* Incosistent Abreviated addresses \n",
    "    * Updated to use consitent values from the mapping dictionary\n",
    "    * Before : 'St', 'Ave'\n",
    "    * After : 'Street' 'Avenue'\n",
    "* Invalid characters \n",
    "    * Invalid characters are ignored\n",
    "    * re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]').match(k_val):\n",
    "* null values and missing attributes\n",
    "    * There are lot of Missing attributes in each node which were ignored.\n",
    "* Incosistent postal codes were present in the data like 豊川市, CA9410, CA\n",
    "    * Removed all trailing and leading spaces\n",
    "    * Removed all non numeric values\n",
    "    * Removed other language values.\n",
    "    * Removed state abbrviations from postcode.\n",
    "    * Before: '豊川市', 'CA9410', 'CA'\n",
    "    * After:'','9410',''\n",
    "* Incosistent city names\n",
    "    * Converted Capital city names to camel case\n",
    "    * Mixed city names to camel case\n",
    "    * Before: 'OAKLAND', 'san francisco', 'daly City'\n",
    "    * After: 'Oakland', 'San francisco', 'Daly City'  \n",
    "    * Code: ' '.join(''.join([w[0].upper(), w[1:].lower()]) for w in st.split())\n",
    "* Invalid position object with lon and lat \n",
    "    * If the lon and lat is not a valid string or not a valid float value initialized to zero\n",
    "    * Code:\n",
    "    try:\n",
    "        json_obj[\"pos\"] = [float(element.get(\"lat\")), float(element.get(\"lon\"))]\n",
    "    except TypeError:  \n",
    "        json_obj[\"pos\"] = [0, 0]\n",
    "* Removed hyphen to extract values which have hypen's in the val.\n",
    "   * Before 'san-francisco'\n",
    "   * After 'San Francisco'\n",
    "* Moved the attributes related to the user to created object.\n",
    "   * Code snipet:\n",
    "   def set_elements(json_obj, element, attributes):\n",
    "    for attribute in attributes:\n",
    "        json_obj[attribute] = element.get(attribute) \n",
    "    set_elements(json_obj[\"created\"],element,[\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"])\n",
    "* Moved address related attributes under addr object\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Other ideas about the dataset\n",
    "* While collecting the data. Marking some of the fields as mandatory will help in improving quality of data. This option will not help in cleaning up the existing data. This will help us in improving the quality of the newly created data.\n",
    "    * Problem with this option would be: Users cannot feed in the data without having the complete data. In some cases users may not have all the attributes. Quality of the data will improve but the quantity of the data may go down.\n",
    "   \n",
    "   \n",
    "* Provide auto completions or drop down to collect consistent data instead of allowing user to type in anything. Even this will help us in improving the quality of the new data.\n",
    "    * Pre populating all the data required for auto completion and drop downs could be an issue. We have to spend more time in getting all the metadata before collecting the actual data\n",
    "\n",
    "\n",
    "* Other ways to improve data is by validating the details with other api's like Google/Yahoo and showing the user with  suggestions to correct before they actually save the data. This option can help us in clean existing dataset and to the all new additions of the dataset.\n",
    "    * Problems with this approach would be that we would be trusting the other services more than the user entered data. In some cases we may repeat the issues which exists in the other services.\n",
    "\n",
    "\n",
    "* Other option would be to get the same data from different users and save multiple copies. Create new data set based on the most matching values. Ex: If three users enter :San Francisco, San Francisco , SF . We take San Francisco as the correct value as most people have entered as San Francisco. Using this option would require more effort in collecting and correcting the data.\n",
    "    * Problems using this approach would be that we would be collecting and storing more than 2x of data than usual. Usually we would be storing 1 record per position but using this approach we have to store many records per position.  \n",
    "\n",
    "\n",
    "* Using Speech to text conversion software to take the input instead of typing in the details. Existing data cannot be corrected using this approach. This will help us to collect data in the future easily.\n",
    "    * There could be a problem recording the data using speech to text because of different languages and different accents. There is as possiblity of recording wrong data and validating the recordings could be complicated.\n",
    "\n",
    "\n",
    "##Data Overview & additional queries\n",
    "* 958.8 MB san-francisco_california.osm\n",
    "* 1.87 GB san-francisco_california.osm.json\n",
    "\n",
    "\n",
    "####Total number of documents\n",
    "* db.sanfrancisco.count()\n",
    "* 4996932\n",
    "\n",
    "####Total number of nodes\n",
    "* db.sanfrancisco.find({'tag':'node'}).count()\n",
    "* 4482203\n",
    "\n",
    "####Total number of ways\n",
    "* db.sanfrancisco.find({'tag':'way'}).count()\n",
    "* 514729\n",
    "\n",
    "####Unique users\n",
    "* db.sanfrancisco.distinct('created.user').length\n",
    "* 3191\n",
    "\n",
    "####Recently created based on time stamp\n",
    "\n",
    "* db.sanfrancisco.aggregate([{$sort:{'created.timestamp':-1}},{$limit:1}])\n",
    "* { \"_id\" : ObjectId(\"5706df8987b8bfa46b49a4bb\"), \"visible\" : null, \"tag\" : \"node\", \"pos\" : [ 37.776917, -122.179906 ], \"id\" : \"4079290925\", \"created\" : { \"uid\" : \"3696217\", \"changeset\" : \"38074868\", \"version\" : \"1\", \"user\" : \"Kristymarie42\", \"timestamp\" : \"2016-03-25T23:50:12Z\" } }\n",
    "\n",
    "####Regex based count\n",
    "* db.sanfrancisco.find({operator:{$regex:'.*way.*'}}).count()\n",
    "* 48\n",
    "\n",
    "####Group by highway \n",
    "* db.sanfrancisco.aggregate([{$group:{'_id':'$highway','count':{$sum:1}}},{$sort:{count:-1}},{$limit:10}])\n",
    "* { \"_id\" : null, \"count\" : 4891251 }\n",
    "{ \"_id\" : \"residential\", \"count\" : 32948 }\n",
    "{ \"_id\" : \"service\", \"count\" : 17360 }\n",
    "{ \"_id\" : \"footway\", \"count\" : 10183 }\n",
    "{ \"_id\" : \"turning_circle\", \"count\" : 5940 }\n",
    "{ \"_id\" : \"secondary\", \"count\" : 4937 }\n",
    "{ \"_id\" : \"crossing\", \"count\" : 3867 }\n",
    "{ \"_id\" : \"tertiary\", \"count\" : 3802 }\n",
    "{ \"_id\" : \"traffic_signals\", \"count\" : 3397 }\n",
    "{ \"_id\" : \"primary\", \"count\" : 3269 }\n",
    "\n",
    "####Group by religion \n",
    "* db.sanfrancisco.aggregate([{$group:{'_id':'$religion','count':{$sum:1}}},{$sort:{count:-1}},{$limit:10}])\n",
    "{ \"_id\" : null, \"count\" : 4995820 }\n",
    "{ \"_id\" : \"christian\", \"count\" : 1036 }\n",
    "{ \"_id\" : \"buddhist\", \"count\" : 33 }\n",
    "{ \"_id\" : \"jewish\", \"count\" : 20 }\n",
    "{ \"_id\" : \"muslim\", \"count\" : 8 }\n",
    "{ \"_id\" : \"taoist\", \"count\" : 3 }\n",
    "{ \"_id\" : \"unitarian_universalist\", \"count\" : 2 }\n",
    "{ \"_id\" : \"unitarian\", \"count\" : 2 }\n",
    "{ \"_id\" : \"scientologist\", \"count\" : 2 }\n",
    "{ \"_id\" : \"eckankar\", \"count\" : 1 }\n",
    "\n",
    "####Group by cuisine \n",
    "* db.sanfrancisco.aggregate([{$group:{'_id':'$cuisine','count':{$sum:1}}},{$sort:{count:-1}},{$limit:2}])\n",
    "{ \"_id\" : null, \"count\" : 4994452 }\n",
    "{ \"_id\" : \"mexican\", \"count\" : 247 }\n",
    "\n",
    "##Others\n",
    "* Residential highway type data has more records than anyother highway type data. \n",
    "* There are 3191 unique users who contributed to the data.\n",
    "* Recently created record was by the user Kristymarie42 at 2016-03-25T23:50:12Z.\n",
    "* Oldest record was created by Deanna Earley at 2006-07-02T20:31:53Z.\n",
    "* Top 3 religions are distributed as\n",
    "    * christian - 1036, buddhist - 33, jewish - 33.\n",
    "* Mexican is the top domination cusine.\n",
    "\n",
    "## Conclusion\n",
    "I feel that the osm data was not very well organized and the naming conventions were not consistent. There was duplication and missing of data in some cases. Difficult moments were with the data set which I choose was big. I had to wait for long time to process every time I run my python scripts for cleaning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
